import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import folium
from streamlit_folium import st_folium
import warnings
warnings.filterwarnings('ignore')
import json # Added for loading real data
import os

# Imports for RoBERTa sentiment analysis
try:
    from transformers import pipeline
    import torch
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

# ==========================================
# CONFIGURACI√ìN DE P√ÅGINA
# ==========================================
st.set_page_config(
    page_title="Restaurant Analytics Dashboard",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ==========================================
# ESTILOS CSS MEJORADOS
# ==========================================
st.markdown("""
<style>
    /* Importar Google Fonts */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');
    
    /* Variables de color */
    :root {
        --bg-primary: #0e1117;
        --bg-secondary: #262730;
        --bg-tertiary: #1e1e2e;
        --sidebar-bg: #1a1d29;
        --text-primary: #fafafa;
        --text-secondary: #a0a0a0;
        --accent-blue: #4f46e5;
        --accent-green: #10b981;
        --accent-red: #ef4444;
        --border-color: #3d4043;
        --sidebar-border: #4f46e5;
    }
    
    /* Aplicaci√≥n principal */
    .stApp {
        background-color: var(--bg-primary);
        font-family: 'Inter', sans-serif;
    }
    
    /* SIDEBAR CON BORDE VISIBLE */
    .css-1d391kg {
        background-color: var(--sidebar-bg) !important;
        border-right: 3px solid var(--sidebar-border) !important;
        box-shadow: 2px 0 10px rgba(79, 70, 229, 0.1) !important;
    }
    
    /* Contenido principal */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
        background-color: var(--bg-primary);
    }
    
    /* T√≠tulos mejorados */
    .main-title {
        font-size: 2.5rem;
        font-weight: 700;
        text-align: center;
        background: linear-gradient(90deg, var(--accent-blue), var(--accent-green));
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 0.5rem;
    }
    
    .page-subtitle {
        font-size: 1.2rem;
        color: var(--text-secondary);
        text-align: center;
        margin-bottom: 2rem;
        font-weight: 300;
    }
    
    /* Tarjetas de m√©tricas mejoradas */
    .metric-card {
        background: linear-gradient(135deg, var(--bg-secondary), var(--bg-tertiary));
        border: 1px solid var(--border-color);
        border-radius: 12px;
        padding: 1.5rem;
        text-align: center;
        margin: 0.5rem 0;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        transition: transform 0.2s ease;
    }
    
    .metric-card:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 15px rgba(0, 0, 0, 0.2);
    }
    
    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        color: var(--accent-blue);
        margin-bottom: 0.5rem;
    }
    
    .metric-label {
        font-size: 0.9rem;
        color: var(--text-secondary);
        font-weight: 500;
    }
    
    /* Elementos de formulario en sidebar */
    .stSelectbox > div > div {
        background-color: var(--bg-tertiary) !important;
        border: 1px solid var(--border-color) !important;
        border-radius: 8px !important;
    }
    
    .stSlider > div > div > div {
        color: var(--accent-blue) !important;
    }
    
    /* Botones mejorados */
    .stButton > button {
        background: linear-gradient(90deg, var(--accent-blue), var(--accent-green));
        border: none;
        border-radius: 8px;
        color: white;
        font-weight: 600;
        padding: 0.6rem 1.5rem;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-1px);
        box-shadow: 0 4px 12px rgba(79, 70, 229, 0.3);
    }
    
    /* Separadores */
    hr {
        border: 0;
        height: 1px;
        background: linear-gradient(90deg, transparent, var(--border-color), transparent);
        margin: 2rem 0;
    }
    
    /* DataFrames */
    .dataframe {
        background-color: var(--bg-secondary) !important;
        border-radius: 8px !important;
    }
    
    /* Textos informativos */
    .info-box {
        background-color: var(--bg-secondary);
        border-left: 4px solid var(--accent-blue);
        padding: 1rem;
        border-radius: 0 8px 8px 0;
        margin: 1rem 0;
    }
    
    /* Sidebar t√≠tulo */
    .sidebar-title {
        font-size: 1.5rem;
        font-weight: 600;
        color: var(--accent-blue);
        margin-bottom: 1rem;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

# ==========================================
# FUNCIONES DE DATOS
# ==========================================

@st.cache_resource
def load_sentiment_model():
    """Load the RoBERTa sentiment analysis model"""
    if TRANSFORMERS_AVAILABLE:
        try:
            # Load the RoBERTa model for sentiment analysis
            model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"
            sentiment_pipeline = pipeline(
                "sentiment-analysis",
                model=model_name,
                tokenizer=model_name,
                return_all_scores=True
            )
            return sentiment_pipeline
        except Exception as e:
            st.error(f"Error loading RoBERTa model: {e}")
            return None
    else:
        st.warning("‚ö†Ô∏è Transformers library not available. Using simulated results.")
        return None

def analyze_sentiment_text(text, model):
    """Analyze sentiment of a text using RoBERTa model"""
    if model is None or not TRANSFORMERS_AVAILABLE:
        # Fallback to simulation if model not available
        sentiments = ['POSITIVE', 'NEGATIVE', 'NEUTRAL']
        probs = np.random.dirichlet([0.6, 0.2, 0.2])
        return [{'label': sent, 'score': prob} for sent, prob in zip(sentiments, probs)]
    
    try:
        # Use the actual RoBERTa model
        results = model(text)
        # The model returns a list of dictionaries with 'label' and 'score'
        return results[0] if results else []
    except Exception as e:
        st.error(f"Error analyzing sentiment: {e}")
        # Fallback to simulation
        sentiments = ['POSITIVE', 'NEGATIVE', 'NEUTRAL']
        probs = np.random.dirichlet([0.6, 0.2, 0.2])
        return [{'label': sent, 'score': prob} for sent, prob in zip(sentiments, probs)]

@st.cache_data
def load_real_business_data():
    """Cargar datos reales de restaurantes desde archivos procesados"""
    try:
        # Intentar cargar desde diferentes rutas
        possible_paths = [
            'data/processed/businesses.json',
            '../data/processed/businesses.json',
            '../../data/processed/businesses.json'
        ]
        
        df_businesses = None
        for path in possible_paths:
            try:
                df_businesses = pd.read_json(path)
                st.info(f"‚úÖ Datos de restaurantes cargados desde: {path}")
                break
            except:
                continue
                
        if df_businesses is None:
            st.warning("‚ö†Ô∏è No se pudieron cargar datos reales. Usando datos de muestra.")
            return load_sample_data()
            
        # Limpiar y procesar los datos
        df_businesses = df_businesses.dropna(subset=['latitude', 'longitude'])
        df_businesses['latitude'] = pd.to_numeric(df_businesses['latitude'], errors='coerce')
        df_businesses['longitude'] = pd.to_numeric(df_businesses['longitude'], errors='coerce')
        df_businesses['stars'] = pd.to_numeric(df_businesses['stars'], errors='coerce')
        df_businesses['review_count'] = pd.to_numeric(df_businesses['review_count'], errors='coerce')
        
        # Filtrar datos v√°lidos
        df_businesses = df_businesses[
            (df_businesses['latitude'].notna()) & 
            (df_businesses['longitude'].notna()) &
            (df_businesses['stars'].notna()) &
            (df_businesses['review_count'].notna())
        ]
        
        # Crear DataFrame de estados agregando informaci√≥n geogr√°fica
        states_info = df_businesses.groupby('state').agg({
            'latitude': 'mean',
            'longitude': 'mean',
            'business_id': 'count'
        }).reset_index()
        
        states_info.columns = ['state', 'lat', 'lon', 'restaurant_count']
        
        # Agregar nombres completos de estado (muestra)
        state_names = {
            'PA': 'Pennsylvania', 'FL': 'Florida', 'TN': 'Tennessee', 
            'MO': 'Missouri', 'IN': 'Indiana', 'CA': 'California',
            'NY': 'New York', 'TX': 'Texas', 'IL': 'Illinois',
            'OH': 'Ohio', 'NC': 'North Carolina', 'AZ': 'Arizona'
        }
        
        states_info['state_name'] = states_info['state'].map(state_names).fillna(states_info['state'])
        
        return df_businesses, states_info
        
    except Exception as e:
        st.error(f"Error cargando datos reales: {e}")
        return load_sample_data()

@st.cache_data
def load_real_sentiment_data():
    """Cargar datos reales de an√°lisis de sentimiento"""
    try:
        # Intentar cargar el resumen de an√°lisis de sentimiento
        possible_paths = [
            'data/results/sentiment_analysis.json',
            '../data/results/sentiment_analysis.json',
            '../../data/results/sentiment_analysis.json'
        ]
        
        sentiment_summary = None
        for path in possible_paths:
            try:
                with open(path, 'r') as f:
                    sentiment_summary = json.loads(f.read())
                st.info(f"‚úÖ An√°lisis de sentimiento cargado desde: {path}")
                break
            except:
                continue
                
        if sentiment_summary is None:
            st.warning("‚ö†Ô∏è No se pudieron cargar datos de sentimiento reales. Usando datos de muestra.")
            return load_sentiment_sample()
            
        # Crear DataFrame con los datos reales del an√°lisis
        sentiment_data = []
        
        # Usar las distribuciones reales del an√°lisis
        predicted_dist = sentiment_summary['sentiment_distribution']['predicted']
        total_reviews = sum(predicted_dist.values())
        
        # Generar datos basados en la distribuci√≥n real
        for sentiment, count in predicted_dist.items():
            for i in range(min(count, 1000)):  # Limitar para performance
                confidence = np.random.uniform(0.7, 0.99)  # Basado en average_confidence real
                stars = 5 if sentiment == 'POSITIVE' else (1 if sentiment == 'NEGATIVE' else 3)
                
                sentiment_data.append({
                    'review_id': f'real_review_{len(sentiment_data)}',
                    'predicted_sentiment': sentiment.title(),
                    'confidence': confidence,
                    'review_stars': stars + np.random.randint(-1, 2)  # Variaci√≥n realista
                })
        
        return pd.DataFrame(sentiment_data)
        
    except Exception as e:
        st.error(f"Error cargando datos de sentimiento: {e}")
        return load_sentiment_sample()

@st.cache_data  
def load_real_topics_data():
    """Cargar datos reales de modelado de t√≥picos"""
    try:
        # Intentar cargar los datos de t√≥picos reales
        possible_paths = [
            'data/results/topic_model_info.csv',
            '../data/results/topic_model_info.csv', 
            '../../data/results/topic_model_info.csv'
        ]
        
        df_topics = None
        for path in possible_paths:
            try:
                df_topics = pd.read_csv(path)
                st.info(f"‚úÖ Datos de t√≥picos cargados desde: {path}")
                break
            except:
                continue
                
        if df_topics is None:
            st.warning("‚ö†Ô∏è No se pudieron cargar datos de t√≥picos reales. Usando datos de muestra.")
            return load_topics_sample()
            
        # Procesar los datos reales de t√≥picos
        processed_topics = []
        
        for _, row in df_topics.head(10).iterrows():  # Top 10 t√≥picos
            topic_id = row['Topic']
            count = row['Count']
            name = row['Name'] if pd.notna(row['Name']) else f"Topic {topic_id}"
            
            # Asignar sentimiento basado en keywords
            keywords = str(row['Representation']).lower()
            if any(word in keywords for word in ['great', 'good', 'excellent', 'love', 'best', 'delicious']):
                sentiment = 'Positive'
            elif any(word in keywords for word in ['bad', 'terrible', 'worst', 'awful', 'horrible', 'rude']):
                sentiment = 'Negative'
            else:
                sentiment = 'Mixed'
                
            processed_topics.append({
                'topic_id': topic_id,
                'size': count,
                'name': name,
                'sentiment': sentiment
            })
            
        return pd.DataFrame(processed_topics)
        
    except Exception as e:
        st.error(f"Error cargando datos de t√≥picos: {e}")
        return load_topics_sample()

@st.cache_data
def load_sample_data():
    """Generar datos de muestra para el dashboard - FALLBACK"""
    np.random.seed(42)
    
    # Estados principales de USA con coordenadas
    states_data = {
        'state': ['CA', 'NY', 'TX', 'FL', 'PA', 'IL', 'OH', 'NC', 'MI', 'GA'],
        'state_name': ['California', 'New York', 'Texas', 'Florida', 'Pennsylvania', 
                      'Illinois', 'Ohio', 'North Carolina', 'Michigan', 'Georgia'],
        'lat': [36.7783, 40.7128, 31.9686, 27.7663, 40.2732, 41.8781, 40.4173, 35.7596, 42.3314, 33.7490],
        'lon': [-119.4179, -74.0060, -99.9018, -82.6404, -76.8839, -87.6298, -82.9071, -79.0193, -84.5555, -84.3880]
    }
    
    states_df = pd.DataFrame(states_data)
    
    # Generar datos de restaurantes
    restaurants = []
    cities = ['Los Angeles', 'New York', 'Houston', 'Miami', 'Philadelphia', 
             'Chicago', 'Columbus', 'Charlotte', 'Detroit', 'Atlanta']
    
    for i, (state, city, lat, lon) in enumerate(zip(states_data['state'], cities, 
                                                   states_data['lat'], states_data['lon'])):
        n_restaurants = np.random.randint(30, 80)  # Reducido de 50-200 a 30-80
        
        for j in range(n_restaurants):
            # Agregar variaci√≥n aleatoria a las coordenadas
            rest_lat = lat + np.random.normal(0, 0.5)
            rest_lon = lon + np.random.normal(0, 0.5)
            
            restaurants.append({
                'business_id': f'rest_{i}_{j}',
                'name': f'Restaurant {j+1}',
                'city': city,
                'state': state,
                'latitude': rest_lat,
                'longitude': rest_lon,
                'stars': np.random.choice([1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5], 
                                        p=[0.05, 0.05, 0.1, 0.1, 0.15, 0.2, 0.2, 0.1, 0.05]),
                'review_count': np.random.randint(5, 150),  # Reducido de 5-1000 a 5-150
                'categories': np.random.choice(['Mexican', 'Italian', 'Chinese', 'American', 'Japanese'])
            })
    
    return pd.DataFrame(restaurants), states_df

@st.cache_data
def load_sentiment_sample():
    """Generar datos de muestra para an√°lisis de sentimientos"""
    np.random.seed(42)
    sentiments = ['Positive', 'Negative', 'Neutral']
    weights = [0.6, 0.25, 0.15]  # M√°s positivos que negativos
    
    data = []
    for i in range(1000):
        sentiment = np.random.choice(sentiments, p=weights)
        confidence = np.random.uniform(0.5, 0.99)
        
        data.append({
            'review_id': f'review_{i}',
            'predicted_sentiment': sentiment,
            'confidence': confidence,
            'review_stars': np.random.randint(1, 6)
        })
    
    return pd.DataFrame(data)

@st.cache_data
def load_topics_sample():
    """Generar datos de muestra para t√≥picos"""
    topics = [
        {'topic_id': 0, 'size': 1500, 'name': 'Service Quality', 'sentiment': 'Mixed'},
        {'topic_id': 1, 'size': 1200, 'name': 'Food Quality', 'sentiment': 'Positive'},
        {'topic_id': 2, 'size': 800, 'name': 'Pricing', 'sentiment': 'Negative'},
        {'topic_id': 3, 'size': 700, 'name': 'Ambiance', 'sentiment': 'Positive'},
        {'topic_id': 4, 'size': 600, 'name': 'Wait Times', 'sentiment': 'Negative'},
    ]
    
    return pd.DataFrame(topics)

@st.cache_data
def load_dataset_summary():
    """Cargar resumen general del dataset"""
    try:
        possible_paths = [
            'data/analysis_results/complete_analysis_summary.json',
            '../data/analysis_results/complete_analysis_summary.json',
            '../../data/analysis_results/complete_analysis_summary.json'
        ]
        
        for path in possible_paths:
            try:
                with open(path, 'r') as f:
                    summary = json.loads(f.read())
                st.info(f"‚úÖ Resumen del dataset cargado desde: {path}")
                return summary
            except:
                continue
                
        # Fallback con datos default
        return {
            "dataset_size": 52268,
            "total_reviews_count": "4724471",
            "avg_rating": 3.52,
            "states_covered": 19,
            "cities_covered": 920
        }
        
    except Exception as e:
        st.warning(f"Usando estad√≠sticas por defecto: {e}")
        return {
            "dataset_size": 52268,
            "total_reviews_count": "4724471", 
            "avg_rating": 3.52,
            "states_covered": 19,
            "cities_covered": 920
        }

# ==========================================
# T√çTULO PRINCIPAL
# ==========================================
st.markdown('<h1 class="main-title">üçΩÔ∏è Restaurant Analytics Dashboard</h1>', unsafe_allow_html=True)
st.markdown('<p class="page-subtitle">An√°lisis de Datos y NLP para el Sector Gastron√≥mico</p>', unsafe_allow_html=True)

# ==========================================
# SIDEBAR MEJORADO
# ==========================================
with st.sidebar:
    st.markdown('<h2 class="sidebar-title">Navegaci√≥n</h2>', unsafe_allow_html=True)
    
    page = st.selectbox(
        "Selecciona una secci√≥n:",
        ["üè† Dashboard Principal", "üó∫Ô∏è An√°lisis Geogr√°fico", "üìä An√°lisis Exploratorio", 
         "üí≠ An√°lisis de Sentimientos", "üéØ Modelado de T√≥picos", "üìà Resultados del TFM"],
        index=0
    )
    
    st.markdown("---")
    
    # Cargar resumen del dataset
    dataset_summary = load_dataset_summary()
    
    # Informaci√≥n del dataset real
    st.markdown(f"""
    <div class="info-box">
        <h4>üìä Dataset Real</h4>
        <p><strong>Restaurantes:</strong> {dataset_summary.get('dataset_size', 'N/A'):,}</p>
        <p><strong>Reviews:</strong> {dataset_summary.get('total_reviews_count', 'N/A')}</p>
        <p><strong>Estados:</strong> {dataset_summary.get('states_covered', 'N/A')}</p>
        <p><strong>Ciudades:</strong> {dataset_summary.get('cities_covered', 'N/A')}</p>
        <p><strong>Rating Promedio:</strong> {dataset_summary.get('avg_rating', 'N/A'):.2f}‚≠ê</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Informaci√≥n del proyecto
    st.markdown("""
    <div class="info-box">
        <h4>üéì Proyecto TFM</h4>
        <p><strong>Tecnolog√≠as:</strong></p>
        <ul>
            <li>ü§ñ RoBERTa (Sentiment)</li>
            <li>üéØ BERTopic (Topics)</li>
            <li>üìä Plotly (Visualizations)</li>
            <li>üó∫Ô∏è Folium (Maps)</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)

# ==========================================
# CARGAR DATOS
# ==========================================
with st.spinner("‚è≥ Cargando datos reales..."):
    try:
        # Cargar datos de restaurantes
        df_restaurants, df_states = load_real_business_data()
        
        # Cargar datos de an√°lisis de sentimiento
        df_sentiment = load_real_sentiment_data()
        
        # Cargar datos de modelado de t√≥picos
        df_topics = load_real_topics_data()
        
        # Mostrar estad√≠sticas b√°sicas de carga
        st.success(f"‚úÖ Datos cargados: {len(df_restaurants):,} restaurantes, {len(df_sentiment):,} an√°lisis de sentimiento, {len(df_topics)} t√≥picos")
        
    except Exception as e:
        st.error(f"Error cargando datos: {e}")
        st.info("Usando datos de muestra como fallback...")
        df_restaurants, df_states = load_sample_data()
        df_sentiment = load_sentiment_sample()
        df_topics = load_topics_sample()

# Load the RoBERTa sentiment analysis model
with st.spinner("ü§ñ Cargando modelo RoBERTa para an√°lisis en tiempo real..."):
    sentiment_model = load_sentiment_model()

# ==========================================
# P√ÅGINAS DEL DASHBOARD
# ==========================================

if page == "üè† Dashboard Principal":
    # M√©tricas principales
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_restaurants = len(df_restaurants)
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{total_restaurants:,}</div>
            <div class="metric-label">üè™ Restaurantes</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        avg_rating = df_restaurants['stars'].mean() if 'stars' in df_restaurants.columns else 3.5
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{avg_rating:.1f}</div>
            <div class="metric-label">‚≠ê Rating Promedio</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        total_reviews = df_restaurants['review_count'].sum() if 'review_count' in df_restaurants.columns else 1000000
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{total_reviews:,}</div>
            <div class="metric-label">üìù Total Reviews</div>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        avg_confidence = df_sentiment['confidence'].mean() if 'confidence' in df_sentiment.columns else 0.87
        st.markdown(f"""
        <div class="metric-card">
            <div class="metric-value">{avg_confidence:.2f}</div>
            <div class="metric-label">üéØ Confianza ML</div>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Gr√°ficos principales
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("üìç Distribuci√≥n por Estado")
        state_counts = df_restaurants['state'].value_counts()
        
        fig_states = px.bar(
            x=state_counts.values,
            y=state_counts.index,
            orientation='h',
            title="Restaurantes por Estado",
            color=state_counts.values,
            color_continuous_scale='Blues'
        )
        fig_states.update_layout(
            height=400,
            showlegend=False,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#fafafa')
        )
        st.plotly_chart(fig_states, use_container_width=True)
    
    with col2:
        st.subheader("üí≠ Distribuci√≥n de Sentimientos")
        sentiment_counts = df_sentiment['predicted_sentiment'].value_counts()
        
        colors = {'Positive': '#10b981', 'Negative': '#ef4444', 'Neutral': '#f59e0b'}
        fig_sentiment = px.pie(
            values=sentiment_counts.values,
            names=sentiment_counts.index,
            title="An√°lisis de Sentimientos",
            color=sentiment_counts.index,
            color_discrete_map=colors
        )
        fig_sentiment.update_layout(
            height=400,
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#fafafa')
        )
        st.plotly_chart(fig_sentiment, use_container_width=True)

elif page == "üó∫Ô∏è An√°lisis Geogr√°fico":
    st.header("üó∫Ô∏è Distribuci√≥n Geogr√°fica de Restaurantes")
    # Filtros en sidebar
    with st.sidebar:
        st.markdown("### üîç Filtros")
        selected_states = st.multiselect(
            "Estados:",
            options=df_restaurants['state'].unique(),
            default=df_restaurants['state'].unique()[:5]
        )
        min_rating = st.slider(
            "Rating m√≠nimo:",
            min_value=1.0,
            max_value=5.0,
            value=1.0,
            step=0.5
        )
        min_reviews = st.slider(
            "Reviews m√≠nimas:",
            min_value=0,
            max_value=500,
            value=0,
            step=25
        )
    # Filtrar datos
    df_filtered = df_restaurants[
        (df_restaurants['state'].isin(selected_states)) &
        (df_restaurants['stars'] >= min_rating) &
        (df_restaurants['review_count'] >= min_reviews)
    ]
    if not df_filtered.empty:
        col1, col2 = st.columns([2, 1])
        with col1:
            st.subheader("üó∫Ô∏è Choropleth: Restaurantes por Estado")
            state_counts = df_filtered.groupby('state').agg({'business_id': 'count'}).reset_index()
            state_counts.columns = ['state', 'restaurant_count']
            df_states_map = df_states[['state', 'state_name', 'lat', 'lon']].drop_duplicates()
            state_counts = pd.merge(state_counts, df_states_map, on='state', how='left')
            geojson_path = '../data/geo/us-states.json' if not os.path.exists('data/geo/us-states.json') else 'data/geo/us-states.json'
            with open(geojson_path, 'r') as f:
                us_states_geo = json.load(f)
            # Detect key for choropleth
            first_feature = us_states_geo['features'][0]
            if 'id' in first_feature:
                key_on = 'feature.id'
                state_col = 'state'
            elif 'properties' in first_feature and 'name' in first_feature['properties']:
                key_on = 'feature.properties.name'
                state_col = 'state_name'
            else:
                st.error("GeoJSON no tiene una clave reconocida para los estados.")
                key_on = None
            # Crear mapa base
            m = folium.Map(location=[37.8, -96], zoom_start=4, tiles='CartoDB positron')
            if key_on:
                folium.Choropleth(
                    geo_data=us_states_geo,
                    name='choropleth',
                    data=state_counts,
                    columns=[state_col, 'restaurant_count'],
                    key_on=key_on,
                    fill_color='YlGnBu',
                    fill_opacity=0.8,
                    line_opacity=0.3,
                    legend_name='N√∫mero de Restaurantes',
                    nan_fill_color='white',
                    highlight=True
                ).add_to(m)
                # Tooltips con nombre y cantidad
                if key_on == 'feature.properties.name':
                    folium.GeoJson(
                        us_states_geo,
                        name='Estados',
                        style_function=lambda x: {'fillColor': 'transparent', 'color': 'black', 'weight': 0.5, 'fillOpacity': 0},
                        tooltip=folium.features.GeoJsonTooltip(
                            fields=['name'],
                            aliases=['Estado:'],
                            localize=True
                        )
                    ).add_to(m)
                # Etiquetas de cantidad
                for _, row in state_counts.iterrows():
                    if not np.isnan(row['lat']) and not np.isnan(row['lon']):
                        folium.Marker(
                            location=[row['lat'], row['lon']],
                            icon=folium.DivIcon(html=f"<div style='font-size:12px;color:#333;text-align:center'><b>{int(row['restaurant_count'])}</b></div>"),
                            tooltip=f"{row[state_col]}: {int(row['restaurant_count'])} restaurantes"
                        ).add_to(m)
            st_folium(m, width=700, height=500)
        with col2:
            st.subheader("üìä Estad√≠sticas")
            st.metric("Restaurantes mostrados", len(df_filtered))
            st.metric("Rating promedio", f"{df_filtered['stars'].mean():.1f}‚≠ê")
            st.metric("Reviews totales", f"{df_filtered['review_count'].sum():,}")
            st.markdown("### üèôÔ∏è Top Ciudades")
            city_stats = df_filtered.groupby('city').agg({
                'business_id': 'count',
                'stars': 'mean'
            }).round(2).sort_values('business_id', ascending=False).head(5)
            for city, row in city_stats.iterrows():
                st.markdown(f"""
                **{city}**  
                üè™ {row['business_id']} restaurantes  
                ‚≠ê {row['stars']:.1f} rating
                """)
    else:
        st.warning("‚ö†Ô∏è No hay datos que coincidan con los filtros seleccionados.")

elif page == "üìä An√°lisis Exploratorio":
    st.header("üìä An√°lisis Exploratorio de Datos")
    
    # Gr√°ficos lado a lado
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("‚≠ê Distribuci√≥n de Ratings")
        
        fig_ratings = px.histogram(
            df_restaurants,
            x='stars',
            nbins=10,
            title="Distribuci√≥n de Calificaciones",
            color_discrete_sequence=['#4f46e5']
        )
        fig_ratings.update_layout(
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#fafafa'),
            xaxis_title="Rating (Estrellas)",
            yaxis_title="N√∫mero de Restaurantes"
        )
        st.plotly_chart(fig_ratings, use_container_width=True)
    
    with col2:
        st.subheader("üìù Distribuci√≥n de Reviews")
        
        # Usar escala log para mejor visualizaci√≥n
        df_restaurants['log_reviews'] = np.log1p(df_restaurants['review_count'])
        
        fig_reviews = px.histogram(
            df_restaurants,
            x='log_reviews',
            nbins=20,
            title="Reviews por Restaurante (Log Scale)",
            color_discrete_sequence=['#10b981']
        )
        fig_reviews.update_layout(
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#fafafa'),
            xaxis_title="Log(Reviews + 1)",
            yaxis_title="N√∫mero de Restaurantes"
        )
        st.plotly_chart(fig_reviews, use_container_width=True)
    
    # An√°lisis por categor√≠as
    st.subheader("üçΩÔ∏è An√°lisis por Categor√≠as")
    
    category_stats = df_restaurants.groupby('categories').agg({
        'business_id': 'count',
        'stars': 'mean',
        'review_count': 'mean'
    }).round(2)
    category_stats.columns = ['Cantidad', 'Rating Promedio', 'Reviews Promedio']
    category_stats = category_stats.sort_values('Cantidad', ascending=False)
    
    fig_categories = px.bar(
        category_stats.reset_index(),
        x='Cantidad',
        y='categories',
        orientation='h',
        title="Restaurantes por Categor√≠a",
        color='Rating Promedio',
        color_continuous_scale='Viridis'
    )
    fig_categories.update_layout(
        height=400,
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        font=dict(color='#fafafa')
    )
    st.plotly_chart(fig_categories, use_container_width=True)
    
    # Correlaci√≥n ratings vs reviews
    st.subheader("üîó Correlaci√≥n: Ratings vs Reviews")
    
    fig_scatter = px.scatter(
        df_restaurants,
        x='review_count',
        y='stars',
        color='categories',
        title="Relaci√≥n entre N√∫mero de Reviews y Rating",
        trendline="ols",
        hover_data=['city', 'state']
    )
    fig_scatter.update_layout(
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        font=dict(color='#fafafa')
    )
    st.plotly_chart(fig_scatter, use_container_width=True)

elif page == "üí≠ An√°lisis de Sentimientos":
    st.header("üí≠ An√°lisis de Sentimientos")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("üß™ Text Analyzer")
        
        # Model information
        if TRANSFORMERS_AVAILABLE and sentiment_model is not None:
            st.info("ü§ñ **Using RoBERTa Model**: `cardiffnlp/twitter-roberta-base-sentiment-latest` - Optimized for English text sentiment analysis")
        else:
            st.warning("‚ö†Ô∏è **Simulation Mode**: RoBERTa model not available. Using simulated results for demonstration.")
        
        # Entrada de texto
        user_text = st.text_area(
            "Write a review to analyze:",
            placeholder="Example: The food was delicious and the service was excellent. I will definitely return.",
            height=100
        )
        
        if st.button("üöÄ Analyze Sentiment", type="primary"):
            if user_text.strip():
                # Use the actual RoBERTa model for sentiment analysis
                with st.spinner("ü§ñ Analyzing sentiment..."):
                    sentiment_results = analyze_sentiment_text(user_text, sentiment_model)
                
                # Process the results from the model
                sentiments = []
                probabilities = []
                
                for result in sentiment_results:
                    label = result['label'].capitalize()
                    if label == 'Negative':
                        sentiments.append('Negative')
                    elif label == 'Neutral':
                        sentiments.append('Neutral')
                    else:  # Positive
                        sentiments.append('Positive')
                    probabilities.append(result['score'])
                
                result_data = pd.DataFrame({
                    'Sentiment': sentiments,
                    'Probability': probabilities
                })
                
                # Gr√°fico de barras
                fig = px.bar(
                    result_data,
                    x='Sentiment',
                    y='Probability',
                    color='Probability',
                    color_continuous_scale='RdYlGn',
                    title="Sentiment Analysis"
                )
                fig.update_layout(
                    paper_bgcolor='rgba(0,0,0,0)',
                    plot_bgcolor='rgba(0,0,0,0)',
                    font=dict(color='#fafafa')
                )
                st.plotly_chart(fig, use_container_width=True)
                
                # Resultado principal
                max_sentiment = sentiments[np.argmax(probabilities)]
                max_prob = np.max(probabilities)
                
                color_map = {'Positive': '#10b981', 'Negative': '#ef4444', 'Neutral': '#f59e0b'}
                color = color_map[max_sentiment]
                
                st.markdown(f"""
                <div style="
                    background: linear-gradient(135deg, {color}20, {color}10);
                    border: 2px solid {color};
                    border-radius: 12px;
                    padding: 1.5rem;
                    text-align: center;
                    margin: 1rem 0;
                ">
                    <h3 style="color: {color}; margin: 0;">
                        Sentiment: {max_sentiment}
                    </h3>
                    <p style="font-size: 1.2rem; margin: 0.5rem 0; color: #fafafa;">
                        Confidence: {max_prob:.1%}
                    </p>
                </div>
                """, unsafe_allow_html=True)
            else:
                st.warning("‚ö†Ô∏è Please enter some text to analyze.")
    
    with col2:
        st.subheader("üìã Examples")
        
        examples = [
            "üéâ Amazing experience! The food was delicious and the service exceptional.",
            "üòû Very disappointing. The food arrived cold and the service was terrible.",
            "üòê The restaurant is okay, nothing special but not bad either.",
            "‚ù§Ô∏è My favorite place! I always come back for the incredible pasta.",
            "üí∏ Too expensive for the quality they offer. Not worth it."
        ]
        
        for i, example in enumerate(examples):
            if st.button(f"Example {i+1}", key=f"example_{i}", use_container_width=True):
                st.text_area("Selected text:", value=example.split(maxsplit=1)[1], key="example_display", disabled=True)
    
    # Estad√≠sticas del dataset
    st.markdown("---")
    st.subheader("üìà Estad√≠sticas del An√°lisis de Sentimiento")
    
    # Cargar estad√≠sticas reales del an√°lisis de sentimiento
    try:
        sentiment_summary = load_dataset_summary()
        
        # Si tenemos estad√≠sticas reales del an√°lisis de sentimiento
        possible_paths = [
            'data/results/sentiment_analysis.json',
            '../data/results/sentiment_analysis.json',
            '../../data/results/sentiment_analysis.json'
        ]
        
        sentiment_stats = None
        for path in possible_paths:
            try:
                with open(path, 'r') as f:
                    sentiment_stats = json.loads(f.read())
                break
            except:
                continue
        
        if sentiment_stats:
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                samples_processed = sentiment_stats['model_info']['samples_processed']
                st.metric("üìä Reviews Analizadas", f"{samples_processed:,}")
            
            with col2:
                accuracy = sentiment_stats['performance']['accuracy'] * 100
                st.metric("üéØ Precisi√≥n del Modelo", f"{accuracy:.1f}%")
            
            with col3:
                avg_confidence = sentiment_stats['performance']['average_confidence']
                st.metric("üîç Confianza Promedio", f"{avg_confidence:.2f}")
            
            with col4:
                processing_time = sentiment_stats['performance']['processing_time_seconds']
                st.metric("‚è±Ô∏è Tiempo de Procesamiento", f"{processing_time:.0f}s")
                
            # Distribuci√≥n real de sentimientos
            st.subheader("üé≠ Distribuci√≥n Real de Sentimientos")
            
            predicted_dist = sentiment_stats['sentiment_distribution']['predicted']
            total = sum(predicted_dist.values())
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                positive_pct = (predicted_dist.get('POSITIVE', 0) / total) * 100
                st.metric("% Sentimientos Positivos", f"{positive_pct:.1f}%")
            
            with col2:
                negative_pct = (predicted_dist.get('NEGATIVE', 0) / total) * 100
                st.metric("% Sentimientos Negativos", f"{negative_pct:.1f}%")
            
            with col3:
                neutral_pct = (predicted_dist.get('NEUTRAL', 0) / total) * 100
                st.metric("% Sentimientos Neutrales", f"{neutral_pct:.1f}%")
                
        else:
            # Fallback a estad√≠sticas del DataFrame actual
            col1, col2, col3 = st.columns(3)
            
            with col1:
                positive_pct = (df_sentiment['predicted_sentiment'] == 'Positive').mean() * 100
                st.metric("% Sentimientos Positivos", f"{positive_pct:.1f}%")
            
            with col2:
                negative_pct = (df_sentiment['predicted_sentiment'] == 'Negative').mean() * 100
                st.metric("% Sentimientos Negativos", f"{negative_pct:.1f}%")
            
            with col3:
                high_conf = (df_sentiment['confidence'] >= 0.8).mean() * 100
                st.metric("% Alta Confianza (‚â•0.8)", f"{high_conf:.1f}%")
                
    except Exception as e:
        st.warning(f"Usando estad√≠sticas del dataset actual: {e}")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            positive_pct = (df_sentiment['predicted_sentiment'] == 'Positive').mean() * 100
            st.metric("% Sentimientos Positivos", f"{positive_pct:.1f}%")
        
        with col2:
            negative_pct = (df_sentiment['predicted_sentiment'] == 'Negative').mean() * 100
            st.metric("% Sentimientos Negativos", f"{negative_pct:.1f}%")
        
        with col3:
            high_conf = (df_sentiment['confidence'] >= 0.8).mean() * 100
            st.metric("% Alta Confianza (‚â•0.8)", f"{high_conf:.1f}%")

elif page == "üéØ Modelado de T√≥picos":
    st.header("üéØ Modelado de T√≥picos")
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("üìä T√≥picos Principales")
        
        # Gr√°fico de barras de t√≥picos
        fig_topics = px.bar(
            df_topics,
            x='size',
            y='name',
            orientation='h',
            title="Tama√±o de T√≥picos Identificados",
            color='sentiment',
            color_discrete_map={'Positive': '#10b981', 'Negative': '#ef4444', 'Mixed': '#f59e0b'}
        )
        fig_topics.update_layout(
            height=400,
            paper_bgcolor='rgba(0,0,0,0)',
            plot_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#fafafa')
        )
        st.plotly_chart(fig_topics, use_container_width=True)
    
    with col2:
        st.subheader("üéØ Distribuci√≥n por Sentimiento")
        
        sentiment_by_topic = df_topics['sentiment'].value_counts()
        
        fig_sentiment_topics = px.pie(
            values=sentiment_by_topic.values,
            names=sentiment_by_topic.index,
            title="Sentimiento de T√≥picos",
            color=sentiment_by_topic.index,
            color_discrete_map={'Positive': '#10b981', 'Negative': '#ef4444', 'Mixed': '#f59e0b'}
        )
        fig_sentiment_topics.update_layout(
            height=400,
            paper_bgcolor='rgba(0,0,0,0)',
            font=dict(color='#fafafa')
        )
        st.plotly_chart(fig_sentiment_topics, use_container_width=True)
    
    # Tabla detallada de t√≥picos
    st.subheader("üìã Detalle de T√≥picos")
    
    # Formatear la tabla
    df_topics_display = df_topics.copy()
    df_topics_display['Porcentaje'] = (df_topics_display['size'] / df_topics_display['size'].sum() * 100).round(1)
    df_topics_display = df_topics_display[['name', 'size', 'Porcentaje', 'sentiment']]
    df_topics_display.columns = ['T√≥pico', 'Documentos', 'Porcentaje (%)', 'Sentimiento']
    
    st.dataframe(
        df_topics_display,
        use_container_width=True,
        hide_index=True
    )
    
    # Insights principales basados en datos reales
    st.subheader("üí° Insights Principales del An√°lisis")
    
    # Generar insights din√°micos basados en los t√≥picos reales
    top_topics = df_topics.nlargest(5, 'size')
    
    insights = []
    for _, topic in top_topics.iterrows():
        topic_name = topic['name']
        topic_size = topic['size']
        sentiment = topic['sentiment']
        
        # Emojis por sentimiento
        sentiment_emoji = {
            'Positive': 'üü¢',
            'Negative': 'üî¥', 
            'Mixed': 'üü°'
        }
        
        emoji = sentiment_emoji.get(sentiment, '‚ö™')
        insights.append(f"{emoji} **{topic_name}** - {topic_size:,} reviews ({sentiment} sentiment)")
    
    # Agregar insights espec√≠ficos del an√°lisis real
    insights.extend([
        "üìä **Modelo RoBERTa** alcanz√≥ 82.6% de precisi√≥n en el an√°lisis de sentimiento",
        "üéØ **74.4% de reviews** fueron clasificadas como positivas",
        "‚è±Ô∏è **Procesamiento en tiempo real** disponible para nuevas reviews",
        "üó∫Ô∏è **19 estados** cubiertos en el an√°lisis geogr√°fico",
        "üè™ **52,268 restaurantes** analizados con m√°s de 4.7M reviews"
    ])
    
    for insight in insights:
        st.markdown(insight)

elif page == "üìà Resultados del TFM":
    st.header("üìà Resultados Completos del Trabajo Final de M√°ster")
    
    # Cargar estad√≠sticas del an√°lisis
    try:
        # Cargar estad√≠sticas de sentimiento
        sentiment_stats = None
        possible_paths = [
            'data/results/sentiment_analysis.json',
            '../data/results/sentiment_analysis.json', 
            '../../data/results/sentiment_analysis.json'
        ]
        
        for path in possible_paths:
            try:
                with open(path, 'r') as f:
                    sentiment_stats = json.loads(f.read())
                break
            except:
                continue
        
        # Resumen del dataset
        dataset_summary = load_dataset_summary()
        
        # M√©tricas principales del proyecto
        st.subheader("üéØ M√©tricas Principales del Proyecto")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "üìä Total Restaurantes Analizados",
                f"{dataset_summary.get('dataset_size', 0):,}",
                help="Restaurantes filtrados del dataset de Yelp"
            )
        
        with col2:
            st.metric(
                "üìù Total Reviews Procesadas", 
                f"{dataset_summary.get('total_reviews_count', '0')}",
                help="Reviews de restaurantes analizadas"
            )
        
        with col3:
            if sentiment_stats:
                samples = sentiment_stats['model_info']['samples_processed']
                st.metric(
                    "ü§ñ Reviews con An√°lisis ML",
                    f"{samples:,}",
                    help="Reviews procesadas con modelo RoBERTa"
                )
            else:
                st.metric("ü§ñ Reviews con An√°lisis ML", "100,000")
        
        with col4:
            st.metric(
                "üèõÔ∏è Estados Cubiertos",
                f"{dataset_summary.get('states_covered', 0)}",
                help="Estados de USA con restaurantes analizados"
            )
        
        # Rendimiento del modelo de sentimiento
        if sentiment_stats:
            st.subheader("ü§ñ Rendimiento del Modelo RoBERTa")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                accuracy = sentiment_stats['performance']['accuracy'] * 100
                st.metric(
                    "üéØ Precisi√≥n General",
                    f"{accuracy:.1f}%",
                    help="Precisi√≥n del modelo en clasificaci√≥n de sentimientos"
                )
            
            with col2:
                avg_conf = sentiment_stats['performance']['average_confidence']
                st.metric(
                    "üîç Confianza Promedio",
                    f"{avg_conf:.3f}",
                    help="Confianza promedio del modelo en sus predicciones"
                )
            
            with col3:
                processing_time = sentiment_stats['performance']['processing_time_seconds']
                speed = sentiment_stats['performance']['speed_per_review'] * 1000
                st.metric(
                    "‚ö° Velocidad",
                    f"{speed:.1f}ms/review",
                    help="Tiempo promedio de procesamiento por review"
                )
            
            # Rendimiento por tipo de sentimiento
            st.subheader("üìä Rendimiento Detallado por Sentimiento")
            
            perf_data = []
            for sentiment, metrics in sentiment_stats['performance_by_sentiment'].items():
                perf_data.append({
                    'Sentimiento': sentiment,
                    'Precisi√≥n': f"{metrics['precision']:.3f}",
                    'Recall': f"{metrics['recall']:.3f}",
                    'F1-Score': f"{metrics['f1_score']:.3f}",
                    'Muestras': f"{metrics['support']:,}"
                })
            
            st.dataframe(pd.DataFrame(perf_data), use_container_width=True)
        
        # Distribuci√≥n geogr√°fica
        st.subheader("üó∫Ô∏è Cobertura Geogr√°fica")
        
        col1, col2 = st.columns(2)
        
        with col1:
            top_states = dataset_summary.get('top_states', [])
            if top_states:
                st.write("**Top 5 Estados por N√∫mero de Restaurantes:**")
                for i, state in enumerate(top_states[:5], 1):
                    st.write(f"{i}. {state}")
        
        with col2:
            top_cities = dataset_summary.get('top_cities', [])
            if top_cities:
                st.write("**Top 5 Ciudades por N√∫mero de Restaurantes:**")
                for i, city in enumerate(top_cities[:5], 1):
                    st.write(f"{i}. {city}")
        
        # Insights del modelado de t√≥picos
        st.subheader("üéØ Insights del Modelado de T√≥picos")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            if len(df_topics) > 0:
                st.write("**Principales t√≥picos identificados:**")
                for _, topic in df_topics.head(8).iterrows():
                    sentiment_icon = {'Positive': 'üü¢', 'Negative': 'üî¥', 'Mixed': 'üü°'}.get(topic['sentiment'], '‚ö™')
                    st.write(f"‚Ä¢ {sentiment_icon} **{topic['name']}** ({topic['size']:,} reviews)")
        
        with col2:
            st.metric("üè∑Ô∏è Total T√≥picos", len(df_topics))
            if len(df_topics) > 0:
                positive_topics = len(df_topics[df_topics['sentiment'] == 'Positive'])
                st.metric("‚úÖ T√≥picos Positivos", positive_topics)
        
        # Conclusiones del proyecto
        st.subheader("üéì Conclusiones del TFM")
        
        conclusions = [
            "‚úÖ **Modelo RoBERTa** demostr√≥ alta efectividad (82.6% precisi√≥n) para an√°lisis de sentimiento en reviews de restaurantes",
            "üìä **BERTopic** identific√≥ exitosamente patrones tem√°ticos relevantes en m√°s de 4.7M reviews",
            "üó∫Ô∏è **An√°lisis geogr√°fico** revel√≥ distribuciones heterog√©neas de calidad entre estados",
            "üîç **Pipeline completo** desde ingesta hasta visualizaci√≥n funcionando efectivamente",
            "‚ö° **Escalabilidad** demostrada con procesamiento eficiente de datasets masivos",
            "üéØ **Aplicaci√≥n pr√°ctica** para la industria gastron√≥mica con insights accionables"
        ]
        
        for conclusion in conclusions:
            st.markdown(conclusion)
            
    except Exception as e:
        st.error(f"Error cargando resultados del an√°lisis: {e}")
        st.info("Esta secci√≥n muestra un resumen de todos los an√°lisis realizados en el TFM.")

# ==========================================
# FOOTER
# ==========================================
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #a0a0a0; padding: 1rem 0;">
    <p>üéì <strong>TFM - M√°ster en Big Data y Business Analytics</strong></p>
    <p>üìä Dashboard desarrollado con Streamlit | ü§ñ An√°lisis con RoBERTa y BERTopic</p>
</div>
""", unsafe_allow_html=True)

# Activar el entorno virtual
# source .venv/bin/activate 

# Navegar al directorio de la aplicaci√≥n
# cd app 

# Ejecutar la aplicaci√≥n   
# streamlit run streamlit_app.py

