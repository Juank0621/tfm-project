{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFM - Ingesta de Datos desde Yelp a MongoDB Atlas\n",
    "\n",
    "Este notebook implementa el pipeline de ingesta de datos para el proyecto:\n",
    "\n",
    "**T√≠tulo del TFM**: An√°lisis de Datos y Procesamiento de Lenguaje Natural para la Extracci√≥n de Opiniones y Modelado de T√≥picos en Restaurantes: Un Enfoque de Big Data y Ciencia de Datos Aplicado al Estudio Integral del Sector Gastron√≥mico\n",
    "\n",
    "## Objetivo del Notebook\n",
    "Implementar un pipeline de datos que:\n",
    "1. Lea los archivos JSON del dataset de Yelp\n",
    "2. Filtre los negocios relevantes (restaurantes)\n",
    "3. Cargue los datos en MongoDB Atlas para su posterior an√°lisis\n",
    "   \n",
    "## Estructura de Datos\n",
    "El dataset de Yelp incluye varios archivos JSON:\n",
    "- `yelp_academic_dataset_business.json`: Informaci√≥n de negocios (ubicaci√≥n, categor√≠as, etc.)\n",
    "- `yelp_academic_dataset_review.json`: Rese√±as de usuarios con texto y calificaciones\n",
    "- `yelp_academic_dataset_user.json`: Informaci√≥n de usuarios\n",
    "- `yelp_academic_dataset_checkin.json`: Check-ins en negocios\n",
    "- `yelp_academic_dataset_tip.json`: Tips cortos de usuarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y Configuraci√≥n\n",
    "\n",
    "Necesitamos las siguientes librer√≠as:\n",
    "- `pymongo`: Para conectar con MongoDB Atlas\n",
    "- `pandas`: Para el manejo eficiente de datos\n",
    "- `tqdm`: Para barras de progreso en operaciones largas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nota sobre las herramientas utilizadas\n",
    "\n",
    "En este notebook usamos:\n",
    "- **uv**: Un instalador de paquetes Python ultrarr√°pido y confiable que reemplaza a pip.\n",
    "- **tqdm**: Para barras de progreso en operaciones de procesamiento.\n",
    "\n",
    "Estas herramientas modernas mejoran la experiencia de desarrollo y la visualizaci√≥n del progreso durante la ejecuci√≥n de tareas largas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalamos las dependencias necesarias con uv (instalador r√°pido de Python)\n",
    "\n",
    "`uv add pymongo tqdm python-dotenv pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conexi√≥n a MongoDB Atlas\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANTE: Seguridad de las Credenciales**\n",
    "- Nunca subas tu contrase√±a a un repositorio\n",
    "- Usa variables de entorno o archivos .env para las credenciales\n",
    "- Aseg√∫rate de que tu IP est√© en la whitelist de MongoDB Atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraci√≥n del archivo .env\n",
    "\n",
    "Para mayor seguridad, es recomendable guardar las credenciales en un archivo `.env` en el directorio ra√≠z del proyecto:\n",
    "\n",
    "```\n",
    "# Archivo .env (coloca este archivo en la ra√≠z del proyecto)\n",
    "MONGODB_PASSWORD=tu_contrase√±a_real\n",
    "```\n",
    "\n",
    "Aseg√∫rate de que este archivo est√© incluido en `.gitignore` para evitar subirlo accidentalmente al repositorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexi√≥n exitosa a MongoDB Atlas\n",
      "üìÅ Base de datos y colecciones configuradas:\n",
      "   - Database: tfm_yelp_db\n",
      "   - Collections: businesses, reviews, clients\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# Cargar variables de entorno desde archivo .env\n",
    "# El archivo .env debe estar en la ra√≠z del proyecto o especificar la ruta\n",
    "dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(\"__file__\"))), '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "# Configuraci√≥n de la conexi√≥n a MongoDB Atlas\n",
    "PASSWORD = os.environ.get(\"MONGODB_PASSWORD\")  # Obtener la contrase√±a del archivo .env\n",
    "\n",
    "# Verificar que la contrase√±a existe\n",
    "if not PASSWORD:\n",
    "    print(\"‚ùå Error: No se encontr√≥ la variable MONGODB_PASSWORD en el archivo .env\")\n",
    "    print(\"Por favor, crea un archivo .env con la variable MONGODB_PASSWORD=tu_contrase√±a_real\")\n",
    "else:\n",
    "    uri = f\"mongodb+srv://juank920621:{PASSWORD}@cluster0.tsbdbxg.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "    # Crear cliente de MongoDB\n",
    "    client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "    try:\n",
    "        # Verificar la conexi√≥n\n",
    "        client.admin.command('ping')\n",
    "        print(\"‚úÖ Conexi√≥n exitosa a MongoDB Atlas\")\n",
    "        \n",
    "        # Crear/seleccionar la base de datos y colecciones\n",
    "        db = client['tfm_yelp_db']\n",
    "        businesses_collection = db['businesses']\n",
    "        reviews_collection = db['reviews']\n",
    "        users_collection = db['clients']\n",
    "        \n",
    "        print(\"üìÅ Base de datos y colecciones configuradas:\")\n",
    "        print(f\"   - Database: {db.name}\")\n",
    "        print(f\"   - Collections: {', '.join([businesses_collection.name, reviews_collection.name, users_collection.name])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error al conectar a MongoDB Atlas:\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaci√≥n de datos JSON a MongoDB Atlas\n",
    "\n",
    "En esta secci√≥n se realiza la importaci√≥n de los archivos principales del *Yelp Open Dataset* (`business`, `review` y `user`) a una base de datos en MongoDB Atlas. Para ello, se utiliza PyMongo y la conexi√≥n segura configurada mediante un archivo `.env` que contiene la contrase√±a.\n",
    "\n",
    "**Pasos realizados:**\n",
    "\n",
    "1. **Conexi√≥n a MongoDB Atlas:**  \n",
    "   Se establece la conexi√≥n usando la URI personalizada, cargando la contrase√±a desde variables de entorno. Se verifica que la conexi√≥n sea exitosa antes de realizar cualquier operaci√≥n.\n",
    "\n",
    "2. **Carga de archivos JSON Lines:**  \n",
    "   Cada uno de los archivos (`yelp_academic_dataset_business.json`, `yelp_academic_dataset_review.json`, `yelp_academic_dataset_user.json`) se encuentra en formato JSON Lines, es decir, cada l√≠nea del archivo representa un documento individual.\n",
    "\n",
    "3. **Importaci√≥n eficiente en lotes:**  \n",
    "   Para evitar problemas de memoria, los documentos se insertan en la base de datos en lotes de 1,000 registros por operaci√≥n. Se utiliza la barra de progreso de `tqdm` para visualizar el avance.\n",
    "\n",
    "4. **Colecciones creadas:**  \n",
    "   - `businesses`: Informaci√≥n sobre los negocios.\n",
    "   - `reviews`: Rese√±as realizadas por los usuarios.\n",
    "   - `clients`: Informaci√≥n de los usuarios.\n",
    "\n",
    "Este proceso deja todos los datos necesarios disponibles en la base de datos MongoDB Atlas para su posterior an√°lisis y explotaci√≥n dentro del proyecto del Trabajo de Fin de M√°ster.\n",
    "\n",
    "> **Nota:** Si los archivos son muy grandes, es recomendable contar con una conexi√≥n estable a internet y evitar ejecutar varias veces la importaci√≥n para no duplicar datos (puede limpiarse la colecci√≥n antes de cada importaci√≥n si es necesario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colecciones limpiadas correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Sup√≥n que ya tienes el cliente y la base de datos conectada (db = client['tfm_yelp_db'])\n",
    "# Limpia cada colecci√≥n relevante (puedes poner esto antes del bucle de importaci√≥n)\n",
    "db['businesses'].delete_many({})\n",
    "db['reviews'].delete_many({})\n",
    "db['clients'].delete_many({})\n",
    "\n",
    "print(\"‚úÖ Colecciones limpiadas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Importando archivo '../data/raw/yelp_academic_dataset_business.json' a colecci√≥n 'businesses' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importando businesses: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150346/150346 [01:46<00:00, 1414.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colecci√≥n 'businesses' importada correctamente con 150346 documentos.\n",
      "\n",
      "üöÄ Importando archivo '../data/raw/yelp_academic_dataset_review.json' a colecci√≥n 'reviews' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importando reviews: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6990280/6990280 [1:15:12<00:00, 1548.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colecci√≥n 'reviews' importada correctamente con 6990280 documentos.\n",
      "\n",
      "üöÄ Importando archivo '../data/raw/yelp_academic_dataset_user.json' a colecci√≥n 'clients' ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importando clients: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1987897/1987897 [46:52<00:00, 706.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Colecci√≥n 'clients' importada correctamente con 1987897 documentos.\n",
      "\n",
      "üéâ Todos los archivos han sido importados con √©xito.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "ARCHIVOS = {\n",
    "    \"businesses\": \"../data/raw/yelp_academic_dataset_business.json\",\n",
    "    \"reviews\": \"../data/raw/yelp_academic_dataset_review.json\",\n",
    "    \"clients\": \"../data/raw/yelp_academic_dataset_user.json\",\n",
    "}\n",
    "\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "for coleccion, ruta in ARCHIVOS.items():\n",
    "    print(f\"\\nüöÄ Importando archivo '{ruta}' a colecci√≥n '{coleccion}' ...\")\n",
    "    collection = db[coleccion]\n",
    "\n",
    "    # (Opcional) Limpiar la colecci√≥n antes de importar\n",
    "    # collection.delete_many({})\n",
    "\n",
    "    # Contar l√≠neas para la barra de progreso\n",
    "    with open(ruta, 'r') as f:\n",
    "        total = sum(1 for _ in f)\n",
    "\n",
    "    with open(ruta, 'r') as f:\n",
    "        batch = []\n",
    "        for line in tqdm(f, total=total, desc=f\"Importando {coleccion}\"):\n",
    "            doc = json.loads(line)\n",
    "            batch.append(doc)\n",
    "            if len(batch) >= BATCH_SIZE:\n",
    "                collection.insert_many(batch)\n",
    "                batch = []\n",
    "        if batch:\n",
    "            collection.insert_many(batch)\n",
    "\n",
    "    print(f\"‚úÖ Colecci√≥n '{coleccion}' importada correctamente con {total} documentos.\")\n",
    "\n",
    "print(\"\\nüéâ Todos los archivos han sido importados con √©xito.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funciones Auxiliares para la Ingesta de Datos\n",
    "\n",
    "Definimos funciones helper para procesar y cargar los datos de manera eficiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.rich import tqdm\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def read_json_in_chunks(file_path, chunk_size=1000):\n",
    "    \"\"\"Lee un archivo JSON l√≠nea por l√≠nea en chunks.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Ruta al archivo JSON\n",
    "        chunk_size: Tama√±o de cada chunk\n",
    "    \"\"\"\n",
    "    chunk = []\n",
    "    \n",
    "    # Abrimos el archivo y procesamos l√≠nea por l√≠nea\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        # Contamos l√≠neas para la barra de progreso (opcional, pero m√°s preciso)\n",
    "        total_lines = sum(1 for _ in open(file_path, 'r', encoding='utf-8'))\n",
    "        \n",
    "        # Usamos tqdm.rich para una barra de progreso visualmente atractiva\n",
    "        for line in tqdm(file, desc=f\"Leyendo {os.path.basename(file_path)}\", \n",
    "                         total=total_lines):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                chunk.append(data)\n",
    "                \n",
    "                if len(chunk) >= chunk_size:\n",
    "                    yield chunk\n",
    "                    chunk = []\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error al decodificar JSON: {e}\")\n",
    "                continue\n",
    "    \n",
    "    if chunk:  # Yield the last chunk if it exists\n",
    "        yield chunk\n",
    "\n",
    "def is_restaurant(business):\n",
    "    \"\"\"Verifica si un negocio es un restaurante basado en sus categor√≠as.\"\"\"\n",
    "    if not business.get('categories'):\n",
    "        return False\n",
    "    categories = business['categories'].lower()\n",
    "    restaurant_keywords = ['restaurant', 'food', 'cafe', 'bar', 'pub', 'bistro', 'diner', \n",
    "                           'pizzeria', 'bakery', 'coffee', 'grill', 'steakhouse', 'sushi', \n",
    "                           'taco', 'burger', 'sandwich', 'BBQ', 'kitchen']\n",
    "    return any(keyword in categories for keyword in restaurant_keywords)\n",
    "\n",
    "def enrich_business_data(business):\n",
    "    \"\"\"Enriquece los datos del negocio con campos adicionales.\"\"\"\n",
    "    business['processed_at'] = datetime.utcnow()\n",
    "    business['is_restaurant'] = is_restaurant(business)\n",
    "    return business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "def analyze_sample_data(collection, sample_size=5, fields_to_show=None):\n",
    "    \"\"\"Analiza una muestra de datos de una colecci√≥n.\n",
    "    \n",
    "    Args:\n",
    "        collection: Colecci√≥n de MongoDB\n",
    "        sample_size: Tama√±o de la muestra\n",
    "        fields_to_show: Lista de campos a mostrar\n",
    "    \"\"\"\n",
    "    sample = list(collection.aggregate([{'$sample': {'size': sample_size}}]))\n",
    "    \n",
    "    if not sample:\n",
    "        print(\"‚ùå No se encontraron documentos en la colecci√≥n.\")\n",
    "        return\n",
    "    \n",
    "    # Si no se especifican campos, mostrar todos\n",
    "    if not fields_to_show:\n",
    "        fields_to_show = list(sample[0].keys())\n",
    "    \n",
    "    # Crear una tabla bonita con Rich\n",
    "    table = Table(title=f\"Muestra de {collection.name}\", show_header=True, header_style=\"bold blue\")\n",
    "    \n",
    "    # A√±adir columnas\n",
    "    for field in fields_to_show:\n",
    "        table.add_column(field, overflow=\"fold\")\n",
    "    \n",
    "    # A√±adir filas\n",
    "    for doc in sample:\n",
    "        row = []\n",
    "        for field in fields_to_show:\n",
    "            if field in doc:\n",
    "                # Truncar valores largos\n",
    "                value = str(doc[field])\n",
    "                if len(value) > 100:\n",
    "                    value = value[:97] + \"...\"\n",
    "                row.append(value)\n",
    "            else:\n",
    "                row.append(\"N/A\")\n",
    "        table.add_row(*row)\n",
    "    \n",
    "    console.print(table)\n",
    "    \n",
    "def export_filtered_data(db, output_file, pipeline=None):\n",
    "    \"\"\"Exporta datos filtrados desde MongoDB a un archivo CSV.\n",
    "    \n",
    "    Args:\n",
    "        db: Base de datos de MongoDB\n",
    "        output_file: Nombre del archivo CSV de salida\n",
    "        pipeline: Pipeline de agregaci√≥n para MongoDB (opcional)\n",
    "    \"\"\"\n",
    "    # Pipeline por defecto: unir restaurantes con sus rese√±as y calcular promedio de estrellas\n",
    "    if pipeline is None:\n",
    "        pipeline = [\n",
    "            # Etapa 1: Obtener solo restaurantes\n",
    "            {'$match': {'is_restaurant': True}},\n",
    "            \n",
    "            # Etapa 2: Lookup para unir con rese√±as\n",
    "            {'$lookup': {\n",
    "                'from': 'reviews',\n",
    "                'localField': 'business_id',\n",
    "                'foreignField': 'business_id',\n",
    "                'as': 'reviews'\n",
    "            }},\n",
    "            \n",
    "            # Etapa 3: A√±adir campos calculados\n",
    "            {'$addFields': {\n",
    "                'avg_rating': {'$avg': '$reviews.stars'},\n",
    "                'review_count': {'$size': '$reviews'},\n",
    "                'has_reviews': {'$gt': [{'$size': '$reviews'}, 0]}\n",
    "            }},\n",
    "            \n",
    "            # Etapa 4: Filtrar los que tienen rese√±as\n",
    "            {'$match': {'has_reviews': True}},\n",
    "            \n",
    "            # Etapa 5: Proyecto solo los campos que nos interesan\n",
    "            {'$project': {\n",
    "                '_id': 0,\n",
    "                'business_id': 1,\n",
    "                'name': 1,\n",
    "                'city': 1,\n",
    "                'state': 1,\n",
    "                'stars': 1,\n",
    "                'avg_rating': 1,\n",
    "                'review_count': 1,\n",
    "                'categories': 1\n",
    "            }}\n",
    "        ]\n",
    "    \n",
    "    # Ejecutar el pipeline y convertir a DataFrame\n",
    "    print(f\"üîç Ejecutando pipeline de agregaci√≥n...\")\n",
    "    result = list(db.businesses.aggregate(pipeline))\n",
    "    \n",
    "    if not result:\n",
    "        print(\"‚ùå No se encontraron resultados.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"‚úÖ Se obtuvieron {len(result):,} documentos.\")\n",
    "    df = pd.DataFrame(result)\n",
    "    \n",
    "    # Guardar a CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"üíæ Datos exportados a {output_file}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_dataframe(df, output_image=None):\n",
    "    \"\"\"Realiza un an√°lisis b√°sico de un DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame de pandas\n",
    "        output_image: Ruta para guardar una imagen con visualizaciones\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä An√°lisis del DataFrame:\")\n",
    "    print(f\"   - Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
    "    print(f\"   - Columnas: {', '.join(df.columns.tolist())}\")\n",
    "    \n",
    "    # Mostrar los primeros registros\n",
    "    print(\"\\nüîç Primeras filas:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Estad√≠sticas descriptivas\n",
    "    print(\"\\nüìà Estad√≠sticas descriptivas:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    # Verificar valores nulos\n",
    "    null_counts = df.isnull().sum()\n",
    "    print(\"\\n‚ùì Valores nulos por columna:\")\n",
    "    display(null_counts[null_counts > 0] if null_counts.any() > 0 else \"No hay valores nulos\")\n",
    "    \n",
    "    # Crear visualizaciones\n",
    "    if 'avg_rating' in df.columns and 'review_count' in df.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df['avg_rating'].dropna(), kde=True)\n",
    "        plt.title('Distribuci√≥n de Calificaciones Promedio')\n",
    "        plt.xlabel('Calificaci√≥n Promedio')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.histplot(df['review_count'].dropna(), kde=True, log_scale=True)\n",
    "        plt.title('Distribuci√≥n de Cantidad de Rese√±as (escala log)')\n",
    "        plt.xlabel('Cantidad de Rese√±as')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if output_image:\n",
    "            plt.savefig(output_image)\n",
    "            print(f\"üì∑ Gr√°ficos guardados en {output_image}\")\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Carga de Datos de Negocios\n",
    "\n",
    "Primero cargamos los datos de negocios, enfoc√°ndonos en restaurantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de rutas\n",
    "BUSINESS_FILE = '../data/raw/yelp_academic_dataset_business.json'\n",
    "\n",
    "# Contadores para estad√≠sticas\n",
    "total_businesses = 0\n",
    "restaurants = 0\n",
    "\n",
    "# Procesar y cargar negocios\n",
    "for chunk in read_json_in_chunks(BUSINESS_FILE):\n",
    "    # Enriquecer datos\n",
    "    enriched_businesses = [enrich_business_data(business) for business in chunk]\n",
    "    \n",
    "    # Filtrar solo restaurantes\n",
    "    restaurant_chunk = [b for b in enriched_businesses if b['is_restaurant']]\n",
    "    \n",
    "    # Actualizar contadores\n",
    "    total_businesses += len(chunk)\n",
    "    restaurants += len(restaurant_chunk)\n",
    "    \n",
    "    # Insertar en MongoDB si hay datos\n",
    "    if restaurant_chunk:\n",
    "        businesses_collection.insert_many(restaurant_chunk)\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas de carga de negocios:\")\n",
    "print(f\"   - Total de negocios procesados: {total_businesses:,}\")\n",
    "print(f\"   - Restaurantes encontrados: {restaurants:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Carga de Rese√±as\n",
    "\n",
    "Ahora cargamos las rese√±as, pero solo aquellas relacionadas con los restaurantes que ya identificamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener IDs de restaurantes\n",
    "restaurant_business_ids = set(businesses_collection.distinct('business_id'))\n",
    "print(f\"üîç Buscando rese√±as para {len(restaurant_business_ids):,} restaurantes\")\n",
    "\n",
    "# Configuraci√≥n\n",
    "REVIEW_FILE = '../data/raw/yelp_academic_dataset_review.json'\n",
    "reviews_processed = 0\n",
    "reviews_restaurants = 0\n",
    "\n",
    "# Procesar rese√±as\n",
    "for chunk in read_json_in_chunks(REVIEW_FILE):\n",
    "    # Filtrar rese√±as de restaurantes\n",
    "    restaurant_reviews = [\n",
    "        {\n",
    "            **review,\n",
    "            'processed_at': datetime.utcnow()\n",
    "        }\n",
    "        for review in chunk\n",
    "        if review['business_id'] in restaurant_business_ids\n",
    "    ]\n",
    "    \n",
    "    # Actualizar contadores\n",
    "    reviews_processed += len(chunk)\n",
    "    reviews_restaurants += len(restaurant_reviews)\n",
    "    \n",
    "    # Insertar en MongoDB si hay datos\n",
    "    if restaurant_reviews:\n",
    "        reviews_collection.insert_many(restaurant_reviews)\n",
    "    \n",
    "    # Mostrar progreso cada 100,000 rese√±as\n",
    "    if reviews_processed % 100_000 == 0:\n",
    "        print(f\"   Procesadas {reviews_processed:,} rese√±as...\")\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas de carga de rese√±as:\")\n",
    "print(f\"   - Total de rese√±as procesadas: {reviews_processed:,}\")\n",
    "print(f\"   - Rese√±as de restaurantes: {reviews_restaurants:,}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Verificaci√≥n Final\n",
    "\n",
    "Comprobamos que los datos se han cargado correctamente en MongoDB:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Carga Completa de los Tres Archivos Principales\n",
    "\n",
    "A continuaci√≥n, implementaremos la carga completa de los tres archivos principales del dataset de Yelp:\n",
    "1. Business\n",
    "2. Reviews\n",
    "3. Users\n",
    "\n",
    "Esto nos permitir√° tener un conjunto de datos completo para realizar an√°lisis m√°s profundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de rutas\n",
    "BUSINESS_FILE = '../data/raw/yelp_academic_dataset_business.json'\n",
    "REVIEW_FILE = '../data/raw/yelp_academic_dataset_review.json'\n",
    "USER_FILE = '../data/raw/yelp_academic_dataset_user.json'\n",
    "\n",
    "# Cargar usuarios relacionados con restaurantes\n",
    "# En este caso, cargamos solo los usuarios que han escrito rese√±as sobre restaurantes\n",
    "print(\"\\nüîÑ Cargando datos de usuarios...\")\n",
    "\n",
    "# 1. Primero obtenemos IDs de usuarios que han escrito rese√±as sobre restaurantes\n",
    "user_ids = set(reviews_collection.distinct('user_id'))\n",
    "print(f\"üîç Encontrados {len(user_ids):,} usuarios con rese√±as en restaurantes\")\n",
    "\n",
    "# 2. Ahora cargamos solo esos usuarios\n",
    "users_processed = 0\n",
    "users_loaded = 0\n",
    "\n",
    "for chunk in read_json_in_chunks(USER_FILE, chunk_size=5000):\n",
    "    # Filtrar usuarios que han escrito rese√±as de restaurantes\n",
    "    restaurant_users = [\n",
    "        {\n",
    "            **user,\n",
    "            'processed_at': datetime.utcnow()\n",
    "        }\n",
    "        for user in chunk\n",
    "        if user['user_id'] in user_ids\n",
    "    ]\n",
    "    \n",
    "    # Actualizar contadores\n",
    "    users_processed += len(chunk)\n",
    "    users_loaded += len(restaurant_users)\n",
    "    \n",
    "    # Insertar en MongoDB si hay datos\n",
    "    if restaurant_users:\n",
    "        users_collection.insert_many(restaurant_users)\n",
    "    \n",
    "    # Mostrar progreso cada 100,000 usuarios\n",
    "    if users_processed % 100_000 == 0:\n",
    "        print(f\"   Procesados {users_processed:,} usuarios...\")\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas de carga de usuarios:\")\n",
    "print(f\"   - Total de usuarios procesados: {users_processed:,}\")\n",
    "print(f\"   - Usuarios con rese√±as en restaurantes: {users_loaded:,}\")\n",
    "\n",
    "# Recuento final de datos en las colecciones\n",
    "print(\"\\nüìä Resumen completo de datos en MongoDB:\")\n",
    "print(f\"   - Total de restaurantes: {businesses_collection.count_documents({'is_restaurant': True}):,}\")\n",
    "print(f\"   - Total de rese√±as de restaurantes: {reviews_collection.count_documents({}):,}\")\n",
    "print(f\"   - Total de usuarios cargados: {users_collection.count_documents({}):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lisis y Exploraci√≥n con PyMongo\n",
    "\n",
    "MongoDB proporciona potentes capacidades de consulta y agregaci√≥n que nos permiten analizar los datos directamente en la base de datos. A continuaci√≥n, exploramos algunos ejemplos de an√°lisis de datos utilizando el framework de agregaci√≥n de MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploremos los datos usando consultas y agregaciones avanzadas de MongoDB\n",
    "\n",
    "# 1. Verificar los campos disponibles en cada colecci√≥n\n",
    "print(\"üìã Campos disponibles en negocios:\")\n",
    "business_fields = list(businesses_collection.find_one({}, {'_id': 0}).keys())\n",
    "print(f\"   {', '.join(business_fields[:10])}...\")\n",
    "\n",
    "print(\"\\nüìã Campos disponibles en rese√±as:\")\n",
    "review_fields = list(reviews_collection.find_one({}, {'_id': 0}).keys())\n",
    "print(f\"   {', '.join(review_fields)}\")\n",
    "\n",
    "print(\"\\nüìã Campos disponibles en usuarios:\")\n",
    "user_fields = list(users_collection.find_one({}, {'_id': 0}).keys())\n",
    "print(f\"   {', '.join(user_fields[:10])}...\")\n",
    "\n",
    "# 2. Analizar una muestra de cada colecci√≥n\n",
    "print(\"\\nüîç Muestra de restaurantes:\")\n",
    "analyze_sample_data(\n",
    "    businesses_collection, \n",
    "    sample_size=3, \n",
    "    fields_to_show=['name', 'city', 'state', 'stars', 'review_count', 'categories']\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Muestra de rese√±as:\")\n",
    "analyze_sample_data(\n",
    "    reviews_collection, \n",
    "    sample_size=3, \n",
    "    fields_to_show=['business_id', 'user_id', 'stars', 'date', 'text']\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Muestra de usuarios:\")\n",
    "analyze_sample_data(\n",
    "    users_collection, \n",
    "    sample_size=3, \n",
    "    fields_to_show=['user_id', 'name', 'review_count', 'yelping_since', 'average_stars']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultas de agregaci√≥n avanzadas\n",
    "\n",
    "# 1. Top 10 ciudades con m√°s restaurantes\n",
    "print(\"\\nüèôÔ∏è Top 10 ciudades con m√°s restaurantes:\")\n",
    "city_pipeline = [\n",
    "    {'$match': {'is_restaurant': True}},\n",
    "    {'$group': {\n",
    "        '_id': {'city': '$city', 'state': '$state'},\n",
    "        'count': {'$sum': 1}\n",
    "    }},\n",
    "    {'$sort': {'count': -1}},\n",
    "    {'$limit': 10}\n",
    "]\n",
    "\n",
    "for result in businesses_collection.aggregate(city_pipeline):\n",
    "    city_info = result['_id']\n",
    "    print(f\"   - {city_info['city']}, {city_info['state']}: {result['count']:,} restaurantes\")\n",
    "\n",
    "# 2. Distribuci√≥n de calificaciones de restaurantes\n",
    "print(\"\\n‚≠ê Distribuci√≥n de calificaciones de restaurantes:\")\n",
    "rating_pipeline = [\n",
    "    {'$match': {'is_restaurant': True}},\n",
    "    {'$group': {\n",
    "        '_id': '$stars',\n",
    "        'count': {'$sum': 1}\n",
    "    }},\n",
    "    {'$sort': {'_id': 1}}\n",
    "]\n",
    "\n",
    "ratings = []\n",
    "counts = []\n",
    "for result in businesses_collection.aggregate(rating_pipeline):\n",
    "    ratings.append(result['_id'])\n",
    "    counts.append(result['count'])\n",
    "    print(f\"   - {result['_id']} estrellas: {result['count']:,} restaurantes\")\n",
    "\n",
    "# 3. Usuarios m√°s activos en rese√±as de restaurantes\n",
    "print(\"\\nüë• Top 5 usuarios con m√°s rese√±as de restaurantes:\")\n",
    "user_pipeline = [\n",
    "    {'$group': {\n",
    "        '_id': '$user_id',\n",
    "        'review_count': {'$sum': 1}\n",
    "    }},\n",
    "    {'$sort': {'review_count': -1}},\n",
    "    {'$limit': 5}\n",
    "]\n",
    "\n",
    "for idx, result in enumerate(reviews_collection.aggregate(user_pipeline), 1):\n",
    "    user = users_collection.find_one({'user_id': result['_id']})\n",
    "    if user:\n",
    "        print(f\"   {idx}. {user['name']}: {result['review_count']:,} rese√±as\")\n",
    "    else:\n",
    "        print(f\"   {idx}. Usuario {result['_id']}: {result['review_count']:,} rese√±as\")\n",
    "\n",
    "# 4. Longitud promedio de rese√±as por calificaci√≥n\n",
    "print(\"\\nüìè Longitud promedio de rese√±as por calificaci√≥n:\")\n",
    "length_pipeline = [\n",
    "    {'$addFields': {\n",
    "        'text_length': {'$strLenCP': '$text'}\n",
    "    }},\n",
    "    {'$group': {\n",
    "        '_id': '$stars',\n",
    "        'avg_length': {'$avg': '$text_length'},\n",
    "        'count': {'$sum': 1}\n",
    "    }},\n",
    "    {'$sort': {'_id': 1}}\n",
    "]\n",
    "\n",
    "for result in reviews_collection.aggregate(length_pipeline):\n",
    "    print(f\"   - {result['_id']} estrellas: {result['avg_length']:.1f} caracteres (basado en {result['count']:,} rese√±as)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exportaci√≥n de Datos para Modelaje\n",
    "\n",
    "Para facilitar el modelaje de los datos, es √∫til exportar un conjunto de datos filtrado y procesado a un archivo CSV. Esto permite utilizar herramientas como pandas, scikit-learn o bibliotecas de NLP para realizar an√°lisis avanzados.\n",
    "\n",
    "A continuaci√≥n, exportaremos un conjunto de datos que incluya informaci√≥n de restaurantes junto con estad√≠sticas de sus rese√±as, que ser√° utilizado posteriormente para modelaje de t√≥picos y an√°lisis de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir pipeline para exportar datos de restaurantes con sus rese√±as\n",
    "# Este pipeline crear√° un dataset enriquecido para modelaje\n",
    "\n",
    "export_pipeline = [\n",
    "    # Etapa 1: Solo restaurantes\n",
    "    {'$match': {'is_restaurant': True}},\n",
    "    \n",
    "    # Etapa 2: Lookup para unir con rese√±as\n",
    "    {'$lookup': {\n",
    "        'from': 'reviews',\n",
    "        'localField': 'business_id',\n",
    "        'foreignField': 'business_id',\n",
    "        'as': 'reviews'\n",
    "    }},\n",
    "    \n",
    "    # Etapa 3: A√±adir campos calculados\n",
    "    {'$addFields': {\n",
    "        'avg_rating': {'$avg': '$reviews.stars'},\n",
    "        'review_count': {'$size': '$reviews'},\n",
    "        'recent_reviews': {\n",
    "            '$slice': [  # Solo incluir las 3 rese√±as m√°s recientes\n",
    "                {'$sortArray': {\n",
    "                    'input': '$reviews',\n",
    "                    'sortBy': {'date': -1}\n",
    "                }},\n",
    "                0, 3\n",
    "            ]\n",
    "        }\n",
    "    }},\n",
    "    \n",
    "    # Etapa 4: Filtrar solo los que tienen al menos 5 rese√±as para tener datos significativos\n",
    "    {'$match': {'review_count': {'$gte': 5}}},\n",
    "    \n",
    "    # Etapa 5: Proyecto final con campos relevantes\n",
    "    {'$project': {\n",
    "        '_id': 0,\n",
    "        'business_id': 1,\n",
    "        'name': 1,\n",
    "        'city': 1,\n",
    "        'state': 1,\n",
    "        'postal_code': 1,\n",
    "        'stars': 1,  # Rating promedio en Yelp\n",
    "        'avg_rating': 1,  # Rating promedio calculado de las rese√±as que tenemos\n",
    "        'review_count': 1,\n",
    "        'categories': 1,\n",
    "        # Extraer textos de las rese√±as recientes\n",
    "        'recent_review_1': {'$arrayElemAt': ['$recent_reviews.text', 0]},\n",
    "        'recent_review_2': {'$arrayElemAt': ['$recent_reviews.text', 1]},\n",
    "        'recent_review_3': {'$arrayElemAt': ['$recent_reviews.text', 2]},\n",
    "        'all_review_count': '$review_count'  # Total de rese√±as en nuestra base\n",
    "    }},\n",
    "    \n",
    "    # Etapa 6: Limitar a 10,000 restaurantes para el archivo de modelo\n",
    "    {'$limit': 10000}\n",
    "]\n",
    "\n",
    "# Crear carpeta de outputs si no existe\n",
    "import os\n",
    "output_dir = '../data/processed'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Exportar datos\n",
    "output_file = os.path.join(output_dir, 'restaurants_for_modeling.csv')\n",
    "model_df = export_filtered_data(db, output_file, export_pipeline)\n",
    "\n",
    "# Analizar el dataset resultante\n",
    "if model_df is not None:\n",
    "    # Verificar dimensiones\n",
    "    print(f\"\\nüìä Dataset para modelaje:\")\n",
    "    print(f\"   - Dimensiones: {model_df.shape[0]} filas x {model_df.shape[1]} columnas\")\n",
    "    \n",
    "    # Ver distribuci√≥n de categor√≠as\n",
    "    if 'categories' in model_df.columns:\n",
    "        # Crear un conjunto de todas las categor√≠as\n",
    "        all_categories = set()\n",
    "        for cats in model_df['categories'].dropna():\n",
    "            if isinstance(cats, str):\n",
    "                all_categories.update([c.strip() for c in cats.split(',')])\n",
    "        \n",
    "        print(f\"   - Se encontraron {len(all_categories)} categor√≠as diferentes\")\n",
    "        print(f\"   - Muestra de categor√≠as: {', '.join(list(all_categories)[:10])}\")\n",
    "    \n",
    "    # Ver los tipos de datos\n",
    "    print(\"\\nüìã Tipos de datos en el dataset:\")\n",
    "    display(model_df.dtypes)\n",
    "    \n",
    "    # Guardar un gr√°fico de an√°lisis\n",
    "    output_image = os.path.join(output_dir, 'restaurants_analysis.png')\n",
    "    analyze_dataframe(model_df, output_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integraci√≥n con Modelaje de Topics y NLP\n",
    "\n",
    "Con el dataset exportado, ahora podemos proceder a la siguiente fase del TFM que incluir√≠a:\n",
    "\n",
    "1. **Preprocesamiento de texto** para el an√°lisis NLP:\n",
    "   - Limpieza de texto (eliminaci√≥n de stopwords, normalizaci√≥n, tokenizaci√≥n)\n",
    "   - Extracci√≥n de caracter√≠sticas (TF-IDF, Word Embeddings)\n",
    "   - An√°lisis de sentimiento en las rese√±as\n",
    "\n",
    "2. **Modelado de t√≥picos** usando t√©cnicas como:\n",
    "   - Latent Dirichlet Allocation (LDA)\n",
    "   - BERTopic\n",
    "   - Top2Vec\n",
    "\n",
    "3. **Visualizaciones interactivas** de los resultados:\n",
    "   - Representaciones de t√≥picos\n",
    "   - Evoluci√≥n del sentimiento por categor√≠a de restaurante\n",
    "   - Mapas geoespaciales de distribuci√≥n de opiniones\n",
    "\n",
    "El archivo CSV generado contiene toda la informaci√≥n necesaria para estos an√°lisis, incluyendo:\n",
    "- Datos b√°sicos de los restaurantes\n",
    "- Categor√≠as para agrupar y segmentar\n",
    "- Texto de rese√±as recientes para an√°lisis NLP\n",
    "- M√©tricas cuantitativas (ratings) para correlaci√≥n con hallazgos de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n b√°sica de algunas t√©cnicas NLP con el dataset exportado\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    \n",
    "    # Primero verificamos si el dataset existe\n",
    "    if 'model_df' in locals() and isinstance(model_df, pd.DataFrame) and len(model_df) > 0:\n",
    "        print(\"üî¨ Demostraci√≥n de t√©cnicas NLP b√°sicas con las rese√±as:\")\n",
    "        \n",
    "        # Crear un corpus de texto combinando las rese√±as recientes\n",
    "        corpus = []\n",
    "        for col in ['recent_review_1', 'recent_review_2', 'recent_review_3']:\n",
    "            if col in model_df.columns:\n",
    "                corpus.extend(model_df[col].dropna().tolist())\n",
    "        \n",
    "        # Eliminar valores nulos y convertir a strings\n",
    "        corpus = [str(text) for text in corpus if text is not None]\n",
    "        \n",
    "        # Imprimir estad√≠sticas del corpus\n",
    "        print(f\"\\nüìä Estad√≠sticas del corpus de rese√±as:\")\n",
    "        print(f\"   - Total de rese√±as en el corpus: {len(corpus):,}\")\n",
    "        print(f\"   - Longitud promedio de las rese√±as: {sum(len(text) for text in corpus) / len(corpus):.1f} caracteres\")\n",
    "        \n",
    "        # Intentar descargar stopwords si no existen\n",
    "        try:\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            \n",
    "            # Unir todo el texto para la nube de palabras\n",
    "            all_text = ' '.join(corpus)\n",
    "            \n",
    "            # Crear y mostrar nube de palabras\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            wordcloud = WordCloud(\n",
    "                width=800, height=400,\n",
    "                background_color='white',\n",
    "                stopwords=stop_words,\n",
    "                max_words=100\n",
    "            ).generate(all_text)\n",
    "            \n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(\"Nube de Palabras de Rese√±as de Restaurantes\", fontsize=20)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Guardar la nube de palabras\n",
    "            wordcloud_path = os.path.join(output_dir, 'reviews_wordcloud.png')\n",
    "            plt.savefig(wordcloud_path)\n",
    "            print(f\"\\nüíæ Nube de palabras guardada en: {wordcloud_path}\")\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è No se pudo generar la nube de palabras: {e}\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No hay un dataset disponible para el an√°lisis NLP\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Para an√°lisis NLP completo, instale las librer√≠as adicionales:\")\n",
    "    print(\"   - wordcloud: para nubes de palabras\")\n",
    "    print(\"   - nltk: para procesamiento de lenguaje natural\")\n",
    "    print(\"   - scikit-learn: para vectorizaci√≥n y modelado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusiones y Resumen del Pipeline de Ingesta\n",
    "\n",
    "‚úÖ **Completado en este notebook**:\n",
    "\n",
    "1. **Ingesta completa de datos**\n",
    "   - Carga selectiva de restaurantes del dataset de Yelp\n",
    "   - Ingesta de rese√±as asociadas a estos restaurantes\n",
    "   - Carga de usuarios que han escrito las rese√±as\n",
    "\n",
    "2. **An√°lisis exploratorio con PyMongo**\n",
    "   - Uso del framework de agregaci√≥n para obtener insights\n",
    "   - An√°lisis de distribuci√≥n geogr√°fica, calificaciones y patrones de rese√±as\n",
    "   - Identificaci√≥n de usuarios m√°s activos y tendencias en el contenido\n",
    "\n",
    "3. **Exportaci√≥n para modelaje**\n",
    "   - Creaci√≥n de un dataset enriquecido para an√°lisis NLP\n",
    "   - Muestra de rese√±as recientes para modelado de t√≥picos\n",
    "   - Inclusi√≥n de campos de metadata para an√°lisis contextual\n",
    "\n",
    "4. **Demo b√°sica de t√©cnicas NLP**\n",
    "   - Procesamiento preliminar del corpus de rese√±as\n",
    "   - Visualizaci√≥n de t√©rminos frecuentes con WordCloud\n",
    "\n",
    "‚û°Ô∏è **Pr√≥ximos Pasos**:\n",
    "\n",
    "1. Profundizar en el an√°lisis exploratorio de datos\n",
    "2. Implementar pipeline de preprocesamiento de texto completo\n",
    "3. Desarrollar modelos de extracci√≥n de t√≥picos y an√°lisis de sentimiento\n",
    "4. Integrar hallazgos en un dashboard interactivo con Streamlit\n",
    "\n",
    "Esta fase de ingesta sienta las bases para todo el an√°lisis posterior, proporcionando un conjunto de datos limpio, filtrado y estructurado para aplicar t√©cnicas avanzadas de NLP y machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas finales\n",
    "print(\"üìä Resumen de datos en MongoDB:\")\n",
    "print(f\"   - Total de restaurantes: {businesses_collection.count_documents({'is_restaurant': True}):,}\")\n",
    "print(f\"   - Total de rese√±as cargadas: {reviews_collection.count_documents({}):,}\")\n",
    "\n",
    "# Ejemplo de agregaci√≥n: Promedio de estrellas por restaurante\n",
    "pipeline = [\n",
    "    {'$group': {\n",
    "        '_id': '$business_id',\n",
    "        'avg_stars': {'$avg': '$stars'},\n",
    "        'review_count': {'$sum': 1}\n",
    "    }},\n",
    "    {'$sort': {'review_count': -1}},\n",
    "    {'$limit': 5}\n",
    "]\n",
    "\n",
    "print(\"\\nüåü Top 5 restaurantes por n√∫mero de rese√±as:\")\n",
    "for result in reviews_collection.aggregate(pipeline):\n",
    "    business = businesses_collection.find_one({'business_id': result['_id']})\n",
    "    if business:\n",
    "        print(f\"   - {business['name']}: {result['review_count']:,} rese√±as, {result['avg_stars']:.1f} estrellas promedio\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Resumen y Pr√≥ximos Pasos\n",
    "\n",
    "‚úÖ **Completado**:\n",
    "- Conexi√≥n exitosa a MongoDB Atlas\n",
    "- Carga de datos de restaurantes\n",
    "- Carga de rese√±as asociadas\n",
    "- Enriquecimiento de datos con campos adicionales\n",
    "\n",
    "‚û°Ô∏è **Pr√≥ximos Pasos**:\n",
    "1. An√°lisis exploratorio de datos (EDA)\n",
    "2. Preprocesamiento de texto para NLP\n",
    "3. Implementaci√≥n de modelos de an√°lisis de sentimiento\n",
    "4. Modelado de t√≥picos con LDA\n",
    "5. Desarrollo del dashboard en Streamlit\n",
    "\n",
    "De acuerdo con el anteproyecto del TFM, estos datos servir√°n para:\n",
    "- Extraer insights valiosos para la toma de decisiones en el sector gastron√≥mico\n",
    "- Aplicar t√©cnicas avanzadas de NLP para an√°lisis de sentimiento y modelado de t√≥picos\n",
    "- Desarrollar un cuadro de mandos interactivo con Streamlit y Plotly\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
